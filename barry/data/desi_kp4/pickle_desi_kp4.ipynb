{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82dae1ab-a0c9-4462-aaca-e3c75b3563cd",
   "metadata": {},
   "source": [
    "# Setting up DESI KP4 files for use in Barry\n",
    "This notebook includes code to ingest all the files for DESI Y1 data and mocks, both Abacus and EZmocks, cutsky and cubic, and for multiple tracers and redshift bins. Barry has a strict set of data inputs, and everything gets pickled up into this format. This means the underlying clustering measurements etc., can be in any format.\n",
    "\n",
    "Barry expects that you will read in/specify and pickle:\n",
    "* The number of correlated datasets. **We set n_data == 1 below, but this would be 2 if we were providing i.e., NGC+SGC data vectors**\n",
    "* Pre and post-recon data power spectrum with 5 multipoles (some multipoles can be set to zero if they are not required/measured). **Pre- and Post-recon currently set to None as there is no real data yet**\n",
    "* N pre and post-recon mock power spectra with 5 multipoles (some can be set to zero if they are not required/measured). **Post-recon currently set to None as there is no post-recon mocks yet**\n",
    "* Pre and post-recon covariance matrices for the power spectra (some elements/blocks can be set to zero if they are not required/measured) **Post-recon currently set to None as there is no post-recon mocks yet**\n",
    "* A fiducial cosmology\n",
    "* A window function convolution matrix (some elements/blocks can be set to the identify matrix if they are not required/measured), corresponding k-binning and integral constraint. Only needed for power spectra\n",
    "* A compression matrix to convert the 3 even multipoles to 5 even+odd (can be given as a block identity matrix if you are not measuring odd multipoles). Only needed for power spectra.\n",
    "\n",
    "Correlation functions are similar but a little simpler (only 3 multipoles, and no window function stuff).\n",
    "\n",
    "The code below reads in all the files for both Abacus and EZmocks for LRGs. Hopefully it can be easily extended to do other tracers and redshift bins. As there is currently no real data, so we just parcel up all the mock measurements and covariance matrices. This is still enough to enable fits to the mock means, and individual realisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2568040c-7a14-4057-93e8-7f63c5905085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9ee4153-b34e-4bef-b4b8-e8c8ad82eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General routines for reading input files, power spectrum, covariance matrix, window function matrix, or odd multipole computa matrix\n",
    "\n",
    "# Correlation function.\n",
    "def getxi(loc, zname):\n",
    "    sin = pd.read_csv(loc+\"/s.txt\", comment=\"#\", skiprows=0, delim_whitespace=True, header=None).to_numpy().T[0]\n",
    "    ximat = [pd.read_csv(loc+f\"/Xi_{ell}\"+zname+\".txt\", comment=\"#\", skiprows=0, delim_whitespace=True, header=None).to_numpy().T for ell in [0,2,4]]\n",
    "    \n",
    "    res = []\n",
    "    nmocks = np.shape(ximat[0])[0]\n",
    "    for i in range(nmocks):\n",
    "        df = {}\n",
    "        df[\"s\"] = sin\n",
    "        for l, ell in enumerate([0, 2, 4]):\n",
    "            df[f\"xi{ell}\"] = ximat[l][i]\n",
    "        res.append(pd.DataFrame(df))\n",
    "        \n",
    "    return res\n",
    "\n",
    "# Power Spectrum. Barry needs 5 even+odd multipoles, but the odd ones can be filled with zeros if these haven't been measured, as is done here.\n",
    "def getpk(loc, zname):\n",
    "    kin = pd.read_csv(loc+\"/k.txt\", comment=\"#\", skiprows=0, delim_whitespace=True, header=None).to_numpy().T[0]\n",
    "    pkmat = [pd.read_csv(loc+f\"/Pk_{ell}\"+zname+\".txt\", comment=\"#\", skiprows=0, delim_whitespace=True, header=None).to_numpy().T for ell in [0,2,4]]\n",
    "    \n",
    "    res = []\n",
    "    nmocks = np.shape(pkmat[0])[0]\n",
    "    for i in range(nmocks):\n",
    "        df = {}\n",
    "        df[\"k\"] = kin\n",
    "        for l, ell in enumerate([0, 2, 4]):\n",
    "            df[f\"pk{ell}\"] = pkmat[l][i]\n",
    "        df[\"pk1\"] = np.zeros(len(df[\"k\"]))\n",
    "        df[\"pk3\"] = np.zeros(len(df[\"k\"]))\n",
    "        res.append(pd.DataFrame(df)[[\"k\", \"pk0\", \"pk1\", \"pk2\", \"pk3\", \"pk4\"]])\n",
    "        \n",
    "    return res\n",
    "\n",
    "# Window function matrix. The window functions are stored in a dictionary of 'step sizes' i.e., how many bins get stuck together relative to the \n",
    "# pk measurements so that we can rebin the P(k) at run time if required. Each step size is a dictionary with:\n",
    "#    the input and output k binning (w_ks_input, w_ks_output), the window function matrix (w_transform) and integral constraint (w_k0_scale).\n",
    "# The window function assumes 5 input and output multipoles. For cubic sims, we can set the integral constraint to zero and window matrix to the identity matrix, as is done here.\n",
    "def getwin_dummy(ks):\n",
    "    res = {\"w_ks_input\": ks.copy(), \"w_k0_scale\": np.zeros(ks.size), \"w_transform\": np.eye(5 * ks.size), \"w_ks_output\": ks.copy()}\n",
    "    return {1: res}  # Step size is one\n",
    "\n",
    "# The conversion matrix M from Beutler 2019. Used to compute the odd multipole models given the even multipoles. In the absence of wide angle effects, or if we don't care about\n",
    "# the odd multipoles, we can set this to a block matrix with identity matrices in the appropriate places, as is done here.\n",
    "def getcomp_dummy(ks):\n",
    "    matrix = np.zeros((5 * ks.size, 3 * ks.size))\n",
    "    matrix[: ks.size, : ks.size] = np.diag(np.ones(ks.size))\n",
    "    matrix[2 * ks.size : 3 * ks.size, ks.size : 2 * ks.size] = np.diag(np.ones(ks.size))\n",
    "    matrix[4 * ks.size :, 2 * ks.size :] = np.diag(np.ones(ks.size))\n",
    "    return matrix\n",
    "\n",
    "# Power spectrum covariance matrix. Needs to have 5 multipoles, but if the some of them haven't been measured, we can set the covariance matrix elements to the identity matrix, as is done here.\n",
    "def format_pk_cov(nks, covfile):\n",
    "\n",
    "    cov_input = pd.read_csv(covfile, comment=\"#\", delim_whitespace=True, header=None).to_numpy()\n",
    "    nin = nks\n",
    "    cov = np.eye(5 * nks)\n",
    "    cov[:nks, :nks] = cov_input[:nks, :nks]\n",
    "    cov[:nks, 2 * nks : 3 * nks] = cov_input[:nks, nin : nin + nks]\n",
    "    cov[:nks, 4 * nks :] = cov_input[:nks, 2 * nin : 2 * nin + nks]\n",
    "    cov[2 * nks : 3 * nks, :nks] = cov_input[nin : nin + nks, :nks]\n",
    "    cov[2 * nks : 3 * nks, 2 * nks : 3 * nks] = cov_input[nin : nin + nks, nin : nin + nks]\n",
    "    cov[2 * nks : 3 * nks, 4 * nks :] = cov_input[nin : nin + nks, 2 * nin : 2 * nin + nks]\n",
    "    cov[4 * nks :, :nks] = cov_input[2 * nin : 2 * nin + nks, :nks]\n",
    "    cov[4 * nks :, 2 * nks : 3 * nks] = cov_input[2 * nin : 2 * nin + nks, nin : nin + nks]\n",
    "    cov[4 * nks :, 4 * nks :] = cov_input[2 * nin : 2 * nin + nks, 2 * nin : 2 * nin + nks]\n",
    "    \n",
    "    # Check the covariance matrix is invertible\n",
    "    v = np.diag(cov_input @ np.linalg.inv(cov_input))\n",
    "    if not np.all(np.isclose(v, 1)):\n",
    "        print(\"ERROR, setting an inappropriate covariance matrix that is almost singular!!!!\")\n",
    "        print(f\"These should all be 1: {v}\")\n",
    "    \n",
    "    # Check the covariance matrix is invertible\n",
    "    v = np.diag(cov @ np.linalg.inv(cov))\n",
    "    if not np.all(np.isclose(v, 1)):\n",
    "        print(\"ERROR, setting an inappropriate covariance matrix that is almost singular!!!!\")\n",
    "        print(f\"These should all be 1: {v}\")\n",
    "    \n",
    "    return cov\n",
    "\n",
    "# Correlation function covariance matrix.\n",
    "def format_xi_cov(nss, covfile):\n",
    "\n",
    "    cov_input = pd.read_csv(covfile, comment=\"#\", delim_whitespace=True, header=None).to_numpy()\n",
    "    nin = nss\n",
    "    cov = np.zeros((3 * nss, 3 * nss))\n",
    "    cov[:nss, :nss] = cov_input[:nss, :nss]\n",
    "    cov[:nss, nss : 2 * nss] = cov_input[:nss, nin : nin + nss]\n",
    "    cov[:nss, 2 * nss :] = cov_input[:nss, 2 * nin : 2 * nin + nss]\n",
    "    cov[nss : 2 * nss, :nss] = cov_input[nin : nin + nss, :nss]\n",
    "    cov[nss : 2 * nss, nss : 2 * nss] = cov_input[nin : nin + nss, nin : nin + nss]\n",
    "    cov[nss : 2 * nss, 2 * nss :] = cov_input[nin : nin + nss, 2 * nin : 2 * nin + nss]\n",
    "    cov[2 * nss :, :nss] = cov_input[2 * nin : 2 * nin + nss, :nss]\n",
    "    cov[2 * nss :, nss : 2 * nss] = cov_input[2 * nin : 2 * nin + nss, nin : nin + nss]\n",
    "    cov[2 * nss :, 2 * nss :] = cov_input[2 * nin : 2 * nin + nss, 2 * nin : 2 * nin + nss]\n",
    "   \n",
    "    # Check the covariance matrix is invertible\n",
    "    v = np.diag(cov_input @ np.linalg.inv(cov_input))\n",
    "    if not np.all(np.isclose(v, 1)):\n",
    "        print(\"ERROR, setting an inappropriate covariance matrix that is almost singular!!!!\")\n",
    "        print(f\"These should all be 1: {v}\")\n",
    "    \n",
    "    # Check the covariance matrix is invertible\n",
    "    v = np.diag(cov @ np.linalg.inv(cov))\n",
    "    if not np.all(np.isclose(v, 1)):\n",
    "        print(\"ERROR, setting an inappropriate covariance matrix that is almost singular!!!!\")\n",
    "        print(f\"These should all be 1: {v}\")\n",
    "\n",
    "    return cov\n",
    "\n",
    "# Useful utility function to collate some Pk data\n",
    "def collect_pk_data(pre_files, post_files, zeff, zname, name):\n",
    "    \n",
    "    pre_res = getpk(pre_files, zname)\n",
    "    #post_res = getpk(post_files, zname)\n",
    "    \n",
    "    ks = pre_res[0][\"k\"].to_numpy()\n",
    "    \n",
    "    pre_cov = format_pk_cov(len(ks), pre_files + \"cov\" + zname + \".txt\")\n",
    "    #post_cov = format_pk_cov(len(ks), post_files + \"cov\" + zname + \".txt\")\n",
    "\n",
    "    split = {\n",
    "        \"n_data\": 1,\n",
    "        \"pre-recon data\": None,\n",
    "        \"pre-recon cov\": pre_cov,\n",
    "        \"post-recon data\": None,\n",
    "        \"post-recon cov\": None,\n",
    "        \"pre-recon mocks\": [v for v in pre_res],\n",
    "        \"post-recon mocks\": None,\n",
    "        \"cosmology\": {\n",
    "            \"om\": (0.1188 + 0.02230 + 0.00064) / 0.6774 ** 2,\n",
    "            \"h0\": 0.6774,\n",
    "            \"z\": zeff,\n",
    "            \"ob\": 0.02230 / 0.6774 ** 2,\n",
    "            \"ns\": 0.9667,\n",
    "            \"mnu\": 0.00064 * 93.14,\n",
    "            \"reconsmoothscale\": 15,\n",
    "        },\n",
    "        \"name\": name,\n",
    "        \"winfit\": getwin_dummy(ks),\n",
    "        \"winpk\": None,  # We can set this to None; Barry will set it to zeroes given the length of the data vector.\n",
    "        \"m_mat\": getcomp_dummy(ks),\n",
    "    }\n",
    "    \n",
    "    with open(f\"../\" + name.lower().replace(\" \", \"_\")+\".pkl\", \"wb\") as f:\n",
    "        pickle.dump(split, f)\n",
    "        \n",
    "    return split\n",
    "\n",
    "# Useful utility function to collate some Xi data\n",
    "def collect_xi_data(pre_files, post_files, zeff, zname, name):\n",
    "\n",
    "    pre_res = getxi(pre_files, zname)\n",
    "    #post_res = getxi(post_files, zname)\n",
    "    \n",
    "    ss = pre_res[0][\"s\"].to_numpy()\n",
    "    \n",
    "    pre_cov = format_xi_cov(len(ss), pre_files + \"cov\" + zname + \".txt\")\n",
    "    #post_cov = format_xi_cov(len(ss), post_files + \"cov\" + zname + \".txt\")\n",
    "\n",
    "    split = {\n",
    "        \"n_data\": 1,\n",
    "        \"pre-recon data\": None,\n",
    "        \"pre-recon cov\": pre_cov,\n",
    "        \"post-recon data\": None,\n",
    "        \"post-recon cov\": None,\n",
    "        \"pre-recon mocks\": [v for v in pre_res],\n",
    "        \"post-recon mocks\": None,\n",
    "        \"cosmology\": {\n",
    "            \"om\": (0.1188 + 0.02230 + 0.00064) / 0.6774 ** 2,\n",
    "            \"h0\": 0.6774,\n",
    "            \"z\": zeff,\n",
    "            \"ob\": 0.02230 / 0.6774 ** 2,\n",
    "            \"ns\": 0.9667,\n",
    "            \"mnu\": 0.00064 * 93.14,\n",
    "            \"reconsmoothscale\": 15,\n",
    "        },\n",
    "        \"name\": name,\n",
    "    }\n",
    "    \n",
    "    with open(f\"../\" + name.lower().replace(\" \", \"_\")+\".pkl\", \"wb\") as f:\n",
    "        pickle.dump(split, f)\n",
    "        \n",
    "    return split\n",
    "\n",
    "# Plot the power spectra, for sanity checking\n",
    "def plot_pk(split):\n",
    "    \n",
    "    color = [\"r\", \"b\", \"g\"]\n",
    "    ks = split[\"pre-recon mocks\"][0][\"k\"]\n",
    "    nmocks = len(split[\"pre-recon mocks\"])\n",
    "    label = [r\"$P_{0}(k)$\", r\"$P_{2}(k)$\", r\"$P_{4}(k)$\"]\n",
    "    for m, pk in enumerate([\"pk0\", \"pk2\", \"pk4\"]):\n",
    "        yerr = ks * np.sqrt(np.diag(split[\"pre-recon cov\"]))[2 * m * len(ks) : (2 * m + 1) * len(ks)]\n",
    "        plt.errorbar(\n",
    "            ks,\n",
    "            ks * np.mean([split[\"pre-recon mocks\"][i][pk] for i in range(nmocks)], axis=0),\n",
    "            yerr=yerr,\n",
    "            marker=\"o\",\n",
    "            ls=\"None\",\n",
    "            c=color[m],\n",
    "            label=label[m],\n",
    "        )\n",
    "        for i in range(nmocks):\n",
    "            plt.errorbar(ks, ks * split[\"pre-recon mocks\"][i][pk], marker=\"None\", ls=\"-\", c='k', alpha=0.1)\n",
    "    plt.xlabel(r\"$k$\")\n",
    "    plt.ylabel(r\"$k\\,P(k)$\")\n",
    "    plt.title(split[\"name\"])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# Plot the power spectra, for sanity checking\n",
    "def plot_xi(split):\n",
    "    \n",
    "    color = [\"r\", \"b\", \"g\"]\n",
    "    ss = split[\"pre-recon mocks\"][0][\"s\"]\n",
    "    nmocks = len(split[\"pre-recon mocks\"])\n",
    "    label = [r\"$\\xi_{0}(k)$\", r\"$\\xi_{2}(k)$\", r\"$\\xi_{4}(k)$\"]\n",
    "    for m, xi in enumerate([\"xi0\", \"xi2\", \"xi4\"]):\n",
    "        yerr = ss ** 2 * np.sqrt(np.diag(split[\"pre-recon cov\"]))[m * len(ss) : (m + 1) * len(ss)]\n",
    "        plt.errorbar(\n",
    "            ss,\n",
    "            ss ** 2 * np.mean([split[\"pre-recon mocks\"][i][xi] for i in range(nmocks)], axis=0),\n",
    "            yerr=yerr,\n",
    "            marker=\"o\",\n",
    "            ls=\"None\",\n",
    "            c=color[m],\n",
    "            label=label[m],\n",
    "        )\n",
    "        for i in range(nmocks):\n",
    "            plt.errorbar(ss, ss ** 2 * split[\"pre-recon mocks\"][i][xi], marker=\"None\", ls=\"-\", c='k', alpha=0.1)\n",
    "    plt.xlabel(r\"$s$\")\n",
    "    plt.ylabel(r\"$s^{2}\\,\\xi(s)$\")\n",
    "    plt.title(split[\"name\"])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# A useful sort function\n",
    "def sortfunc(item):\n",
    "    if \"Abacus\" in item:\n",
    "        return int(item.split(\"_\")[7][2:])\n",
    "    else:\n",
    "        return int(item.split(\"_\")[11] if \"recon\" in item else item.split(\"_\")[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccf4ba9-3f4f-4208-9aa1-f62187bc2d77",
   "metadata": {},
   "source": [
    "# Abacus Cubic Mocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "163e147d-326f-43d1-a2a9-243c7dd890d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['k', 'pk0', 'pk1', 'pk2', 'pk3', 'pk4'], dtype='object')] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-753cb5028da9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpre_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"Pk/jmena/nmesh_512/dk0.005/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"DESI KP4 Abacus CubicBox Pk \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_pk_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mplot_pk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Plot the data to check things\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ecc984fe78cf>\u001b[0m in \u001b[0;36mcollect_pk_data\u001b[0;34m(pre_files, post_files, zeff, zname, name)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcollect_pk_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mpre_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0;31m#post_res = getpk(post_files, zname)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ecc984fe78cf>\u001b[0m in \u001b[0;36mgetpk\u001b[0;34m(loc, zname)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pk1\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"k\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pk3\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"k\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"k\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pk0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pk1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pk2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pk3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pk4\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/common/software/python/3.7-anaconda-2019.07/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/common/software/python/3.7-anaconda-2019.07/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1900\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot index with multidimensional key'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1902\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m             \u001b[0;31m# nested tuple slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/common/software/python/3.7-anaconda-2019.07/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1203\u001b[0m             \u001b[0;31m# A collection of keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m             keyarr, indexer = self._get_listlike_indexer(key, axis,\n\u001b[0;32m-> 1205\u001b[0;31m                                                          raise_missing=False)\n\u001b[0m\u001b[1;32m   1206\u001b[0m             return self.obj._reindex_with_indexers({axis: [keyarr, indexer]},\n\u001b[1;32m   1207\u001b[0m                                                    copy=True, allow_dups=True)\n",
      "\u001b[0;32m/usr/common/software/python/3.7-anaconda-2019.07/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         self._validate_read_indexer(keyarr, indexer,\n\u001b[1;32m   1160\u001b[0m                                     \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                                     raise_missing=raise_missing)\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/common/software/python/3.7-anaconda-2019.07/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1244\u001b[0m                 raise KeyError(\n\u001b[1;32m   1245\u001b[0m                     u\"None of [{key}] are in the [{axis}]\".format(\n\u001b[0;32m-> 1246\u001b[0;31m                         key=key, axis=self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['k', 'pk0', 'pk1', 'pk2', 'pk3', 'pk4'], dtype='object')] are in the [index]\""
     ]
    }
   ],
   "source": [
    "# Loop over tracers\n",
    "path = \"/global/cfs/cdirs/desi/cosmosim/KP45/MC/Clustering/AbacusSummit/CubicBox/\"\n",
    "for tracer in [\"LRG\"]:\n",
    "    \n",
    "    # Power Spectrum\n",
    "    pre_file = path + \"Pk/jmena/nmesh_512/dk0.005/\"\n",
    "    name = f\"DESI KP4 Abacus CubicBox Pk \" + tracer\n",
    "    data = collect_pk_data(pre_file, None, 0.8, \"\", name)\n",
    "    plot_pk(data) # Plot the data to check things\n",
    "   \n",
    "    # Correlation Function\n",
    "    pre_file = path + \"Xi/jmena/\"\n",
    "    name = f\"DESI KP4 Abacus CubicBox Xi \" + tracer\n",
    "    data = collect_xi_data(pre_file, None, 0.8, \"\", name)\n",
    "    plot_xi(data) # Plot the data to check things"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c9bae9-60c9-41e9-8179-59ae5d7e45f1",
   "metadata": {},
   "source": [
    "# Abacus CutSky Mocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d371e6-cbb9-4540-b202-76955883a6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over tracers and redshift bins\n",
    "path = \"/global/cfs/cdirs/desi/cosmosim/KP45/MC/Clustering/AbacusSummit/CutSky/\"\n",
    "\n",
    "# Dictionary containing zmin, zmax and zeff for the tracers\n",
    "reds = {\"LRG\": [[\"0.4\", \"0.6\", 0.5], [\"0.6\", \"0.8\", 0.7], [\"0.8\", \"1.1\", 0.95]]}\n",
    "\n",
    "for tracer in [\"LRG\"]:\n",
    "    for i, (zmin, zmax, zeff) in enumerate(reds[tracer]):\n",
    "        \n",
    "        # Power Spectrum\n",
    "        pre_file = path + tracer + \"/Pk/jmena/nmesh_1024/dk0.005/\"\n",
    "        zname = \"_zmin\" + zmin + \"_zmax\" + zmax\n",
    "        name = f\"DESI KP4 Abacus CutSky Pk \" + tracer + zname\n",
    "        data = collect_pk_data(pre_file, None, zeff, zname, name)\n",
    "        plot_pk(data) # Plot the data to check things\n",
    "\n",
    "        # Correlation Function\n",
    "        pre_file = path + tracer + \"/Xi/jmena/\"\n",
    "        zname = \"_zmin\" + zmin + \"_zmax\" + zmax\n",
    "        name = f\"DESI KP4 Abacus CutSky Xi \" + tracer + zname\n",
    "        data = collect_xi_data(pre_file, None, zeff, zname, name)\n",
    "        plot_xi(data) # Plot the data to check things"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6910bb-88b6-4dee-a289-a7dc3373f8d2",
   "metadata": {},
   "source": [
    "# EZmocks CutSky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3a86d5-0f70-4d7d-a31b-5d7361783af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over tracers and redshift bins\n",
    "path = \"/global/cfs/cdirs/desi/cosmosim/KP45/MC/Clustering/EZmock/CutSky/\"\n",
    "\n",
    "# Dictionary containing zmin, zmax and zeff for the tracers\n",
    "reds = {\"LRG\": [[\"0.4\", \"0.6\", 0.5], [\"0.6\", \"0.8\", 0.7], [\"0.8\", \"1.1\", 0.95]]}\n",
    "\n",
    "for tracer in [\"LRG\"]:\n",
    "    for i, (zmin, zmax, zeff) in enumerate(reds[tracer]):\n",
    "                \n",
    "        # Power Spectrum\n",
    "        pre_file = path + tracer + \"/Pk/jmena/nmesh_1024/dk0.005/\"\n",
    "        zname = \"_zmin\" + zmin + \"_zmax\" + zmax\n",
    "        name = f\"DESI KP4 EZmock CutSky Pk \" + tracer + zname\n",
    "        data = collect_pk_data(pre_file, None, zeff, zname, name)\n",
    "        plot_pk(data) # Plot the data to check things\n",
    "\n",
    "        # Correlation Function\n",
    "        #pre_file = path + tracer + \"/Xi/jmena/\"\n",
    "        #zname = \"_zmin\" + zmin + \"_zmax\" + zmax\n",
    "        #name = f\"DESI KP4 EZmock CutSky Pk \" + tracer + zname\n",
    "        #data = collect_pk_data(pre_file, None, zeff, zname, name)\n",
    "        #plot_pk(data) # Plot the data to check things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee4cb9e-d91b-434f-a032-f2e3c198f567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
